{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"datasets/sixray/train_data\"\n",
    "\n",
    "# Define the transformation to resize the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor()  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "class Preprocessed_data(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        self.patch_size = (56,56)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        original_img = self.dataset[index][0]\n",
    "        noise = np.random.normal(0,10**5,size=original_img.shape)\n",
    "        noisy_img = original_img+noise\n",
    "\n",
    "        row, col, ch =  noisy_img.shape\n",
    "        patches = []\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                if (i+1)*56 <= row and (j+1)*56 <= col:\n",
    "                    patch = noisy_img[(i)*56:(i+1)*56,(j)*56:(j+1)*56,:]\n",
    "                    patches.append(patch)\n",
    "\n",
    "\n",
    "        return patches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.dataset))\n",
    "\n",
    "preproc_dataset = Preprocessed_data(dataset)\n",
    "# Create the data loader\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(preproc_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 1\n",
    "            \n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 2\n",
    "            \n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer 3\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 1\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 2\n",
    "            \n",
    "            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')  # Upsampling layer 3\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the autoencoder model\n",
    "model = Autoencoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the autoencoder model\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.6248\n",
      "Epoch [2/30], Loss: 0.3546\n",
      "Epoch [3/30], Loss: 0.1560\n",
      "Epoch [4/30], Loss: 0.1283\n",
      "Epoch [5/30], Loss: 0.1155\n",
      "Epoch [6/30], Loss: 0.1037\n",
      "Epoch [7/30], Loss: 0.0962\n",
      "Epoch [8/30], Loss: 0.0920\n",
      "Epoch [9/30], Loss: 0.0872\n",
      "Epoch [10/30], Loss: 0.0845\n",
      "Epoch [11/30], Loss: 0.0806\n",
      "Epoch [12/30], Loss: 0.0764\n",
      "Epoch [13/30], Loss: 0.0754\n",
      "Epoch [14/30], Loss: 0.0738\n",
      "Epoch [15/30], Loss: 0.0703\n",
      "Epoch [16/30], Loss: 0.0711\n",
      "Epoch [17/30], Loss: 0.0712\n",
      "Epoch [18/30], Loss: 0.0685\n",
      "Epoch [19/30], Loss: 0.0666\n",
      "Epoch [20/30], Loss: 0.0650\n",
      "Epoch [21/30], Loss: 0.0625\n",
      "Epoch [22/30], Loss: 0.0603\n",
      "Epoch [23/30], Loss: 0.0590\n",
      "Epoch [24/30], Loss: 0.0572\n",
      "Epoch [25/30], Loss: 0.0572\n",
      "Epoch [26/30], Loss: 0.0562\n",
      "Epoch [27/30], Loss: 0.0557\n",
      "Epoch [28/30], Loss: 0.0543\n",
      "Epoch [29/30], Loss: 0.0558\n",
      "Epoch [30/30], Loss: 0.0544\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, _ in data_loader:\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        #loss = criterion(outputs, images)\n",
    "        # Calculate MAE loss\n",
    "        mae_loss = nn.L1Loss()(outputs, images)\n",
    "        \n",
    "        # Calculate MSE loss\n",
    "        mse_loss = nn.MSELoss()(outputs, images)\n",
    "        \n",
    "        # Compute the combined loss\n",
    "        loss = 0.7 * mae_loss + 0.3 * mse_loss\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"autoencoder_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     72\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mfor\u001b[39;00m images, _ \u001b[39min\u001b[39;00m test_data_loader:\n\u001b[0;32m     74\u001b[0m         \u001b[39m# Move images to the device\u001b[39;00m\n\u001b[0;32m     75\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m         \u001b[39m# Reconstruct images\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the path to the test dataset\n",
    "test_dataset_path = \"datasets/sixray/test_data\"\n",
    "\n",
    "# Define the transformation to resize the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor()  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = ImageFolder(test_dataset_path, transform=transform)\n",
    "preproc_test = Preprocessed_data(test_dataset)\n",
    "\n",
    "# Create the data loader for test images\n",
    "batch_size = 1  # We process one image at a time\n",
    "test_data_loader = DataLoader(preproc_test, batch_size=batch_size)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 1\n",
    "            \n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 2\n",
    "            \n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer 3\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 1\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 2\n",
    "            \n",
    "            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')  # Upsampling layer 3\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the autoencoder model\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(\"autoencoder_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Reconstruct and plot the images\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for images, _ in test_data_loader:\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Reconstruct images\n",
    "        outputs = model(images)\n",
    "\n",
    "        disparity = images - outputs\n",
    "        \n",
    "        \n",
    "        # Move images and outputs back to CPU\n",
    "        images = images.cpu()\n",
    "        outputs = outputs.cpu()\n",
    "        disparity = disparity.cpu()\n",
    "        \n",
    "        # Plot the original and reconstructed images\n",
    "        original_img = transforms.ToPILImage()(images[0])\n",
    "        reconstructed_img = transforms.ToPILImage()(outputs[0])\n",
    "        disp = transforms.ToPILImage()(disparity[0])\n",
    "        \n",
    "        original_img = np.array(original_img)\n",
    "        original_img = cv.cvtColor(original_img,cv.COLOR_RGB2BGR)\n",
    "        cv.imwrite(\"datasets/sixray/results/real/\"+ str(i) + \".jpg\",original_img)\n",
    "\n",
    "        reconstructed_img = np.array(reconstructed_img)\n",
    "        reconstructed_img = cv.cvtColor(reconstructed_img,cv.COLOR_RGB2BGR)\n",
    "        cv.imwrite(\"datasets/sixray/results/fake/\"+ str(i) + \".jpg\",reconstructed_img)\n",
    "\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "        axs[0].imshow(original_img)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "        \n",
    "        axs[1].imshow(reconstructed_img)\n",
    "        axs[1].set_title(\"Reconstructed Image\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(disp)\n",
    "        axs[2].set_title(\"Disparity Maps\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
