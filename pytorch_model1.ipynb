{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"datasets/sixray/train_data\"\n",
    "\n",
    "# Define the transformation to resize the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1120, 1120)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor()  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ImageFolder(dataset_path, transform=transform)\n",
    "\n",
    "class Preprocessed_data(Dataset):\n",
    "    def __init__(self,dataset):\n",
    "        self.dataset = dataset\n",
    "        self.patch_size = (112,112)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        original_img = self.dataset[index][0]\n",
    "        noise = np.random.normal(0,10**5,size=original_img.shape)\n",
    "        noisy_img = original_img+noise\n",
    "\n",
    "        row, col, ch =  noisy_img.shape\n",
    "        patches = []\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                if (i+1)*56 <= row and (j+1)*56 <= col:\n",
    "                    patch = noisy_img[(i)*56:(i+1)*56,(j)*56:(j+1)*56,:]\n",
    "                    patches.append(patch)\n",
    "\n",
    "\n",
    "        return patches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.dataset))\n",
    "\n",
    "#preproc_dataset = Preprocessed_data(dataset)\n",
    "# Create the data loader\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "print(len(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 1\n",
    "            \n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 2\n",
    "            \n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer 3\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 1\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 2\n",
    "            \n",
    "            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')  # Upsampling layer 3\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the autoencoder model\n",
    "model = Autoencoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the autoencoder model\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     10\u001b[0m \u001b[39m#loss = criterion(outputs, images)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# Calculate MAE loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m mae_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mL1Loss()(outputs, images)\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[37], line 40\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 40\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     41\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[0;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\harih\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, _ in data_loader:\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        #loss = criterion(outputs, images)\n",
    "        # Calculate MAE loss\n",
    "        mae_loss = nn.L1Loss()(outputs, images)\n",
    "        \n",
    "        # Calculate MSE loss\n",
    "        mse_loss = nn.MSELoss()(outputs, images)\n",
    "        \n",
    "        # Compute the combined loss\n",
    "        loss = 0.7 * mae_loss + 0.3 * mse_loss\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"autoencoder_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     72\u001b[0m     i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mfor\u001b[39;00m images, _ \u001b[39min\u001b[39;00m test_data_loader:\n\u001b[0;32m     74\u001b[0m         \u001b[39m# Move images to the device\u001b[39;00m\n\u001b[0;32m     75\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m         \u001b[39m# Reconstruct images\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the path to the test dataset\n",
    "test_dataset_path = \"datasets/sixray/test_data\"\n",
    "\n",
    "# Define the transformation to resize the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor()  # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Create the test dataset\n",
    "test_dataset = ImageFolder(test_dataset_path, transform=transform)\n",
    "preproc_test = Preprocessed_data(test_dataset)\n",
    "\n",
    "# Create the data loader for test images\n",
    "batch_size = 1  # We process one image at a time\n",
    "test_data_loader = DataLoader(preproc_test, batch_size=batch_size)\n",
    "\n",
    "# Define the autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 1\n",
    "            \n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max pooling layer 2\n",
    "            \n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling layer 3\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, padding=1),  # Layer 1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 1\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),  # Layer 2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # Upsampling layer 2\n",
    "            \n",
    "            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),  # Layer 3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')  # Upsampling layer 3\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the autoencoder model\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(\"autoencoder_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Reconstruct and plot the images\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for images, _ in test_data_loader:\n",
    "        # Move images to the device\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Reconstruct images\n",
    "        outputs = model(images)\n",
    "\n",
    "        disparity = images - outputs\n",
    "        \n",
    "        \n",
    "        # Move images and outputs back to CPU\n",
    "        images = images.cpu()\n",
    "        outputs = outputs.cpu()\n",
    "        disparity = disparity.cpu()\n",
    "        \n",
    "        # Plot the original and reconstructed images\n",
    "        original_img = transforms.ToPILImage()(images[0])\n",
    "        reconstructed_img = transforms.ToPILImage()(outputs[0])\n",
    "        disp = transforms.ToPILImage()(disparity[0])\n",
    "        \n",
    "        original_img = np.array(original_img)\n",
    "        original_img = cv.cvtColor(original_img,cv.COLOR_RGB2BGR)\n",
    "        cv.imwrite(\"datasets/sixray/results/real/\"+ str(i) + \".jpg\",original_img)\n",
    "\n",
    "        reconstructed_img = np.array(reconstructed_img)\n",
    "        reconstructed_img = cv.cvtColor(reconstructed_img,cv.COLOR_RGB2BGR)\n",
    "        cv.imwrite(\"datasets/sixray/results/fake/\"+ str(i) + \".jpg\",reconstructed_img)\n",
    "\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "        axs[0].imshow(original_img)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "        \n",
    "        axs[1].imshow(reconstructed_img)\n",
    "        axs[1].set_title(\"Reconstructed Image\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(disp)\n",
    "        axs[2].set_title(\"Disparity Maps\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
